{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T12:53:53.796714500Z",
     "start_time": "2024-03-27T12:53:53.288173700Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch_geometric import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from fastcore.all import *\n",
    "from copy import deepcopy\n",
    "import hdbscan\n",
    "from torch_geometric.utils import degree\n",
    "from environment_manager import *\n",
    "from fastcore.all import *\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "import networkx as nx\n",
    "from typing import List, Tuple\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from copy import deepcopy\n",
    "\n",
    "env = load_create_environment('240318-initial parsing')\n",
    "GHComponentTable.initialise()\n",
    "component_idxes = GHComponentTable.df\n",
    "guid_to_idx_dict = {id: i for i, id in enumerate(GHComponentTable.df['guid'])}\n",
    "idx_to_guid_dict = {i: id for i, id in enumerate(GHComponentTable.df['guid'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Transform Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac7fc563aeddfe4f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_and_assign_centralities(G, compute_degree=False, compute_closeness=False, compute_betweenness=False, compute_in_out_degree=False,compute_katz=False, compute_pagerank=False, compute_harmonic=False, compute_clustering=False, compute_coreness=False,  compute_hdbscan=False, hdbscan_min_cluster_size=5, **kwargs):\n",
    "    if compute_degree:\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        nx.set_node_attributes(G, degree_centrality, 'degree_centrality')\n",
    "    \n",
    "    if compute_closeness:\n",
    "        closeness_centrality = nx.closeness_centrality(G)\n",
    "        nx.set_node_attributes(G, closeness_centrality, 'closeness_centrality')\n",
    "    \n",
    "    if compute_betweenness:\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        nx.set_node_attributes(G, betweenness_centrality, 'betweenness_centrality')\n",
    "\n",
    "    \n",
    "    if compute_katz:\n",
    "        katz_centrality = nx.katz_centrality(G, **kwargs.get('katz', {}))\n",
    "        nx.set_node_attributes(G, katz_centrality, 'katz_centrality')\n",
    "    \n",
    "    if compute_pagerank:\n",
    "        pagerank = nx.pagerank(G, **kwargs.get('pagerank', {}))\n",
    "        nx.set_node_attributes(G, pagerank, 'pagerank')\n",
    "    \n",
    "    if compute_harmonic:\n",
    "        harmonic_centrality = nx.harmonic_centrality(G)\n",
    "        nx.set_node_attributes(G, harmonic_centrality, 'harmonic_centrality')\n",
    "    \n",
    "    if compute_clustering:\n",
    "        clustering_coefficient = nx.clustering(G)\n",
    "        nx.set_node_attributes(G, clustering_coefficient, 'clustering_coefficient')\n",
    "    \n",
    "    if compute_coreness:\n",
    "        # Directly assign the k-core number for each node without using max()\n",
    "        coreness = nx.core_number(G)\n",
    "        nx.set_node_attributes(G, coreness, 'coreness')\n",
    "\n",
    "    \n",
    "    if compute_in_out_degree:\n",
    "        in_degree_centrality = nx.in_degree_centrality(G)\n",
    "        out_degree_centrality = nx.out_degree_centrality(G)\n",
    "        nx.set_node_attributes(G, in_degree_centrality, 'in_degree_centrality')\n",
    "        nx.set_node_attributes(G, out_degree_centrality, 'out_degree_centrality')\n",
    "        \n",
    "    if compute_hdbscan:\n",
    "           # Extract node positions\n",
    "           positions = np.array([(G.nodes[node]['x'], G.nodes[node]['y']) for node in G.nodes()])\n",
    "           \n",
    "           # Apply HDBSCAN on spatial coordinates\n",
    "           # Ensure that any DBSCAN-specific kwargs are not passed to HDBSCAN\n",
    "           hdbscan_kwargs = {k: v for k, v in kwargs.items() if k.startswith('hdbscan_')}\n",
    "           clustering = hdbscan.HDBSCAN(min_cluster_size=hdbscan_min_cluster_size, **hdbscan_kwargs).fit(positions)\n",
    "           \n",
    "           # Assign cluster labels to nodes\n",
    "           for node, label in zip(G.nodes(), clustering.labels_):\n",
    "               G.nodes[node]['hdbscan_cluster'] = label\n",
    "\n",
    "def trnsfrm_positions(data):\n",
    "    x_ = StandardScaler().fit_transform(np.array(data['x']).reshape(-1, 1))\n",
    "    y_ = StandardScaler().fit_transform(np.array(data['y']).reshape(-1, 1))\n",
    "    data['x'] = torch.tensor(np.concatenate([x_, y_], axis=1), dtype=torch.float32)\n",
    "\n",
    "def trnsfm_compid(data, guid_to_idx_dict):\n",
    "    assert data['x'] is not None, 'Please run trnsfrm_positions first'\n",
    "    data['y'] = torch.tensor([guid_to_idx_dict[ids] for ids in data['compid']])\n",
    "    ohe = OneHotEncoder(categories=[range(len(guid_to_idx_dict.values()))], handle_unknown='ignore')\n",
    "    data['y'] = torch.tensor(ohe.fit_transform(data['y'].reshape(-1, 1)).toarray())\n",
    "    return data\n",
    "\n",
    "def trnsform_edges(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.tensor(data.toarray())\n",
    "    t = torch.concat([torch.from_numpy(data[x]) for x in ['s_paramidx', 'r_paramidx', 's_access', 'r_access', 's_datamapping', 'r_datamapping']], dim=1)\n",
    "    return t\n",
    "\n",
    "def trnsfrm_paramidx(data):\n",
    "    ohe = OneHotEncoder(categories=[range(11)], handle_unknown='ignore')\n",
    "    p_in = ohe.fit_transform(np.array(data['s_paramidx'].reshape(-1, 1)))\n",
    "    p_out = ohe.fit_transform(np.array(data['r_paramidx'].reshape(-1, 1)))\n",
    "    data['s_paramidx'] = p_in.toarray()\n",
    "    data['r_paramidx'] = p_out.toarray()\n",
    "\n",
    "def trnsfrm_access(data):\n",
    "    ohe = OneHotEncoder(categories=[range(3)], handle_unknown='ignore')\n",
    "    s_access = ohe.fit_transform(np.array(data['s_access'].reshape(-1, 1)))\n",
    "    r_access = ohe.fit_transform(np.array(data['r_access'].reshape(-1, 1)))\n",
    "    data['s_access'] = s_access.toarray()\n",
    "    data['r_access'] = r_access.toarray()\n",
    "\n",
    "def trnsfrm_datamapping(data):\n",
    "    ohe = OneHotEncoder(categories=[range(3)], handle_unknown='ignore')\n",
    "    s_datamapping = ohe.fit_transform(np.array(data['s_datamapping'].reshape(-1, 1)))\n",
    "    r_datamapping = ohe.fit_transform(np.array(data['r_datamapping'].reshape(-1, 1)))\n",
    "    data['s_datamapping'] = s_datamapping.toarray()\n",
    "    data['r_datamapping'] = r_datamapping.toarray()\n",
    "\n",
    "def trsnfrm_dist(data):\n",
    "    scaler = StandardScaler()\n",
    "    data['distance'] = scaler.fit_transform(np.array(data['distance']).reshape(-1, 1))\n",
    "\n",
    "def processX(data):\n",
    "    std = {'x', 'edge_index', 'y', 'edge_features'}\n",
    "    for feature in data.keys():  # Call keys() method to get an iterable view of the keys\n",
    "        if feature in std:\n",
    "            continue\n",
    "        if data[feature].shape[0] == data['x'].shape[0]:\n",
    "            # Apply scaler and reshape the feature to have the same dimension as 'x'\n",
    "            if feature not in {'in_degree_centrality', 'out_degree_centrality'}:\n",
    "                data[feature] = torch.tensor(StandardScaler().fit_transform(np.array(data[feature]).reshape(-1, 1)), dtype=torch.float32).view(-1)\n",
    "            data['x'] = torch.cat([data['x'], data[feature].view(-1, 1)], dim=1)\n",
    "            del data[feature]\n",
    "def trnsfrm(data, guid_to_idx_dict, rmv_categories=None):\n",
    "    if rmv_categories is None:\n",
    "        rmv_categories = ['compid', 's_paramidx', 'r_paramidx', 's_access', 'r_access', 's_datamapping', 'r_datamapping', 'distance', 'r_is_optional', 's_is_optional', 'category']\n",
    "    data_ = deepcopy(data)\n",
    "\n",
    "    trnsfrm_positions(data_)\n",
    "    trnsfm_compid(data_, guid_to_idx_dict)\n",
    "    trnsfrm_paramidx(data_)\n",
    "    trnsfrm_access(data_)\n",
    "    trnsfrm_datamapping(data_)\n",
    "    trsnfrm_dist(data_)\n",
    "    data_['edge_features'] = trnsform_edges(data_)\n",
    "    for ftr in rmv_categories:\n",
    "        del data_[ftr]\n",
    "\n",
    "    processX(data_)\n",
    "\n",
    "    return data_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T13:04:06.964685Z",
     "start_time": "2024-03-27T13:04:06.937288Z"
    }
   },
   "id": "ea1f3b1f09ebaf",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphMLDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, graphml_files: List[str] = None, num_files: int = None, transform=None, pre_transform=None):\n",
    "        super().__init__(root_dir, transform, pre_transform)\n",
    "        if graphml_files is None:\n",
    "            self.graphml_files = [file for file in os.listdir(root_dir) if file.endswith(\".graphml\")]\n",
    "            if num_files is not None:\n",
    "                self.graphml_files = random.sample(self.graphml_files, num_files)\n",
    "        else:\n",
    "            self.graphml_files = graphml_files\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return self.graphml_files\n",
    "\n",
    "    def len(self) -> int:\n",
    "        return len(self.graphml_files)\n",
    "\n",
    "    def get(self, idx: int) -> Data:\n",
    "        graphml_file = self.graphml_files[idx]\n",
    "        graphml_path = os.path.join(self.root, graphml_file)\n",
    "        graph = nx.read_graphml(graphml_path)\n",
    "\n",
    "        # Remove isolated nodes\n",
    "        graph.remove_nodes_from(list(nx.isolates(graph)))\n",
    "\n",
    "        # Compute and assign centralities\n",
    "        compute_and_assign_centralities(graph, compute_betweenness=True, compute_in_out_degree=True)\n",
    "\n",
    "        data = utils.from_networkx(graph)\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_dataloader(root_dir: str, guid_to_idx_dict, batch_size: int, graphml_files: List[str] = None, num_files: int = None, shuffle: bool = True) -> Tuple[DataLoader, DataLoader]:\n",
    "    dataset = GraphMLDataset(root_dir, graphml_files=graphml_files, num_files=num_files, transform=lambda data: trnsfrm(data, guid_to_idx_dict))\n",
    "    num_nodes = sum(data.num_nodes for data in dataset)\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    train_size = int(num_nodes * 0.7)\n",
    "    val_size = int(num_nodes * 0.2)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = deepcopy(train_mask)\n",
    "    test_mask = deepcopy(train_mask)\n",
    "\n",
    "    train_mask[perm[:train_size]] = True\n",
    "    val_mask[perm[train_size:train_size+val_size]] = True\n",
    "    test_mask[perm[train_size+val_size:]] = True\n",
    "\n",
    "    train_dataset = dataset[train_mask]\n",
    "    val_dataset = dataset[val_mask]\n",
    "    test_dataset = dataset[test_mask]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# # Usage example\n",
    "# root_directory = r\"C:\\Users\\jossi\\Dropbox\\Office_Work\\Jos\\GH_Graph_Learning\\ExtractionEnvironments\\240318-initial parsing\\05-GraphML\\01599_Sketch ModelsJS.graphml\"\n",
    "# guid_to_idx_dict = {id: i for i, id in enumerate(GHComponentTable.df['guid'])}\n",
    "# batch_size = 32\n",
    "# \n",
    "# train_loader, val_loader, test_loader = get_dataloader(root_directory, guid_to_idx_dict, batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T13:04:07.880248200Z",
     "start_time": "2024-03-27T13:04:07.874231400Z"
    }
   },
   "id": "51715c755c5029d9",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Load a single GraphML file\u001B[39;00m\n\u001B[0;32m      7\u001B[0m single_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01599_Sketch ModelsJS.graphml\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 8\u001B[0m train_loader, val_loader, test_loader \u001B[38;5;241m=\u001B[39m \u001B[43mget_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mguid_to_idx_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraphml_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43msingle_file\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# # Load all GraphML files from a folder\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# train_loader, val_loader, test_loader = get_dataloader(root_directory, guid_to_idx_dict, batch_size)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# # Load a random subset of GraphML files from a folder\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# num_files = 10\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# train_loader, val_loader, test_loader = get_dataloader(root_directory, guid_to_idx_dict, batch_size, num_files=num_files)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[16], line 47\u001B[0m, in \u001B[0;36mget_dataloader\u001B[1;34m(root_dir, guid_to_idx_dict, batch_size, graphml_files, num_files, shuffle)\u001B[0m\n\u001B[0;32m     44\u001B[0m val_mask[perm[train_size:train_size\u001B[38;5;241m+\u001B[39mval_size]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     45\u001B[0m test_mask[perm[train_size\u001B[38;5;241m+\u001B[39mval_size:]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtrain_mask\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     48\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m dataset[val_mask]\n\u001B[0;32m     49\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m dataset[test_mask]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gnns\\lib\\site-packages\\torch_geometric\\data\\dataset.py:294\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_select\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gnns\\lib\\site-packages\\torch_geometric\\data\\dataset.py:324\u001B[0m, in \u001B[0;36mDataset.index_select\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, Tensor) \u001B[38;5;129;01mand\u001B[39;00m idx\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mbool:\n\u001B[0;32m    323\u001B[0m     idx \u001B[38;5;241m=\u001B[39m idx\u001B[38;5;241m.\u001B[39mflatten()\u001B[38;5;241m.\u001B[39mnonzero(as_tuple\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_select\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m idx\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mint64:\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_select(idx\u001B[38;5;241m.\u001B[39mflatten()\u001B[38;5;241m.\u001B[39mtolist())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gnns\\lib\\site-packages\\torch_geometric\\data\\dataset.py:334\u001B[0m, in \u001B[0;36mDataset.index_select\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_select(idx\u001B[38;5;241m.\u001B[39mflatten()\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, Sequence) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 334\u001B[0m     indices \u001B[38;5;241m=\u001B[39m [indices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]\n\u001B[0;32m    336\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    337\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[0;32m    338\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly slices (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m), list, tuples, torch.tensor and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    339\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray of dtype long or bool are valid indices (got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    340\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(idx)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gnns\\lib\\site-packages\\torch_geometric\\data\\dataset.py:334\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_select(idx\u001B[38;5;241m.\u001B[39mflatten()\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, Sequence) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 334\u001B[0m     indices \u001B[38;5;241m=\u001B[39m [\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]\n\u001B[0;32m    336\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    337\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[0;32m    338\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly slices (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m), list, tuples, torch.tensor and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    339\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray of dtype long or bool are valid indices (got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    340\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(idx)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: range object index out of range"
     ]
    }
   ],
   "source": [
    "# Usage examples\n",
    "root_directory = env.dirs['graphml']\n",
    "guid_to_idx_dict = {id: i for i, id in enumerate(GHComponentTable.df['guid'])}\n",
    "batch_size = 32\n",
    "\n",
    "# Load a single GraphML file\n",
    "single_file = \"01599_Sketch ModelsJS.graphml\"\n",
    "train_loader, val_loader, test_loader = get_dataloader(root_directory, guid_to_idx_dict, batch_size, graphml_files=[single_file])\n",
    "\n",
    "# # Load all GraphML files from a folder\n",
    "# train_loader, val_loader, test_loader = get_dataloader(root_directory, guid_to_idx_dict, batch_size)\n",
    "# \n",
    "# # Load a random subset of GraphML files from a folder\n",
    "# num_files = 10\n",
    "# train_loader, val_loader, test_loader = get_dataloader(root_directory, guid_to_idx_dict, batch_size, num_files=num_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T13:04:09.620215400Z",
     "start_time": "2024-03-27T13:04:08.922295500Z"
    }
   },
   "id": "69b12a81e126bb31",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ebadadaa410e92a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
